import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import platform
import os
from matplotlib import font_manager, rcParams

# WordCloudãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from wordcloud import WordCloud

# Plotly Expressã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import plotly.express as px

# NetworkXã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ (ã‚°ãƒ©ãƒ•æç”»ã®ãŸã‚)
import networkx as nx

# Streamlit Plotly Eventsã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from streamlit_plotly_events import plotly_events

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š (WordCloudã¨Matplotlibç”¨)

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ãƒ•ã‚©ãƒ³ãƒˆãƒ‘ã‚¹ã‚’æŒ‡å®š
# ä¾‹ãˆã°ã€ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã« 'fonts' ãƒ•ã‚©ãƒ«ãƒ€ãŒã‚ã‚Šã€ãã®ä¸­ã« 'NotoSansJP-Regular.ttf' ãŒã‚ã‚‹å ´åˆ
font_file_name = 'NotoSansJP-VariableFont_wght.ttf' # ã¾ãŸã¯ä½¿ç”¨ã—ãŸã„Noto Sans JPã®ãƒ•ã‚¡ã‚¤ãƒ«å

# fonts ãƒ•ã‚©ãƒ«ãƒ€å†…ã«ã‚ã‚‹ã‹ã€ã¾ãŸã¯ç›´æ¥ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚ã‚‹ã‹ã‚’ç¢ºèª
if os.path.exists(f'fonts/{font_file_name}'):
    font_path = f'fonts/{font_file_name}'
elif os.path.exists(font_file_name): # ç›´æ¥ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚ã‚‹å ´åˆ
    font_path = font_file_name
else:
    # æœ€çµ‚çš„ãªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆãƒ‡ãƒ—ãƒ­ã‚¤ç’°å¢ƒã§ã¯é€šå¸¸ã“ã®ãƒ‘ã‚¹ã¯å­˜åœ¨ã—ãªã„ã¯ãšã§ã™ãŒã€å¿µã®ãŸã‚ï¼‰
    # ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºæ™‚ã®å‚è€ƒç”¨ã¨ã—ã¦æ®‹ã—ã¦ãŠã
    st.warning(f"æŒ‡å®šã•ã‚ŒãŸåŒæ¢±ãƒ•ã‚©ãƒ³ãƒˆ '{font_file_name}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚·ã‚¹ãƒ†ãƒ ãƒ•ã‚©ãƒ³ãƒˆã‚’è©¦ã—ã¾ã™ã€‚")
    if platform.system() == 'Windows':
        font_path = 'C:/Windows/Fonts/meiryo.ttc'
    elif platform.system() == 'Darwin':
        font_path = '/System/Library/Fonts/ãƒ’ãƒ©ã‚®ãƒè§’ã‚´ã‚·ãƒƒã‚¯ W3.ttc'
    else:
        font_path = '/usr/share/fonts/truetype/vlgothic/VL-Gothic-Regular.ttf'


font_prop = None
if os.path.exists(font_path):
    font_prop = font_manager.FontProperties(fname=font_path)
    rcParams['font.family'] = font_prop.get_name()
    st.success(f"ãƒ•ã‚©ãƒ³ãƒˆ '{font_prop.get_name()}' ã‚’è¨­å®šã—ã¾ã—ãŸã€‚")
else:
    st.error(f"æŒ‡å®šã•ã‚ŒãŸæ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆ '{font_path}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚æ–‡å­—åŒ–ã‘ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
    # ã“ã“ã«åˆ°é”ã—ãŸå ´åˆã€ãƒ•ã‚©ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸã“ã¨ã‚’æ˜ç¢ºã«ã™ã‚‹
    rcParams['font.family'] = ['sans-serif'] # æœ€çµ‚çš„ãªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        
warnings.filterwarnings('ignore')

from collections import Counter
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from janome.tokenizer import Tokenizer
import re
from itertools import combinations
import json

class SurveyNLPAnalyzer:
    def __init__(self, df):
        self.df = df.copy() # ã‚ªãƒªã‚¸ãƒŠãƒ«DataFrameã®ã‚³ãƒ”ãƒ¼ã‚’æ“ä½œ
        self.tokenizer = Tokenizer()
        self.analysis_results = {}
        self.question_cols = [col for col in self.df.columns if re.match(r'Q\d+å›ç­”', col)]

        self.non_specific_patterns = [
            r'ã‚ã‹ã‚‰ãªã„', r'ç‰¹ã«ãªã—', r'ä¸æ˜', r'ãªã—',
            r'ç‰¹ã«.*ãªã„', r'ã‚ˆãã‚ã‹ã‚‰ãªã„', r'^\s*$', r'ãªã„'
        ]
        self.excluded_verbs = {'ã™ã‚‹', 'ãªã‚‹', 'ã„ã‚‹', 'ã‚‰ã‚Œã‚‹', 'æ„Ÿã˜ã‚‹', 'ã‚„ã™ã„', 'ã›ã‚‹', 'ã‚ã‚‹', 'ã„ã†', 'è‡ªåˆ†'}
        
        self.wc_font_path = font_path # Word Cloudç”¨ã®ãƒ•ã‚©ãƒ³ãƒˆãƒ‘ã‚¹

    def preprocess_text(self, text):
        if pd.isna(text):
            return ""
        text_str = str(text).lower()
        for pattern in self.non_specific_patterns:
            if re.search(pattern, text_str):
                return ""
        text_str = re.sub(r'[ï¼!ï¼Ÿ?ã€‚ï¼ã€ï¼Œ\s\tã€€]+', ' ', text_str)
        text_str = re.sub(r'[^\wã-ã‚“ã‚¡-ãƒ³ä¸€-é¾¥a-zA-Z0-9]', '', text_str)
        return text_str.strip()

    def morphological_analysis(self, text):
        if not text:
            return []
        words = []
        for token in self.tokenizer.tokenize(text):
            base_form = token.base_form
            pos = token.part_of_speech.split(',')[0]
            
            if pos in ['åè©', 'å‹•è©', 'å½¢å®¹è©']:
                if len(base_form) > 1 and base_form not in self.excluded_verbs:
                    words.append(base_form)
        return words

    def extract_keywords_tfidf(self, texts, top_n=10):
        processed_texts_for_vectorizer = [" ".join(self.morphological_analysis(text)) for text in texts if text]
        
        if not processed_texts_for_vectorizer:
            return []
        
        vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2), min_df=1, max_df=0.8)
        tfidf_matrix = vectorizer.fit_transform(processed_texts_for_vectorizer)
        
        feature_names = vectorizer.get_feature_names_out()
        scores = tfidf_matrix.sum(axis=0).A1
        keywords = [(feature_names[i], scores[i]) for i in range(len(feature_names))]
        return sorted(keywords, key=lambda x: x[1], reverse=True)[:top_n]

    def topic_modeling_lda(self, texts, n_topics=5):
        processed_texts_for_vectorizer = [" ".join(self.morphological_analysis(text)) for text in texts if text]

        if len(processed_texts_for_vectorizer) < 2:
            return []
        try:
            vectorizer = TfidfVectorizer(max_features=100, min_df=1, max_df=0.8)
            tfidf_matrix = vectorizer.fit_transform(processed_texts_for_vectorizer)
            
            feature_names = vectorizer.get_feature_names_out()
            lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)
            lda.fit(tfidf_matrix)
            topics = []
            for topic_idx, topic in enumerate(lda.components_):
                top_words = [feature_names[i] for i in topic.argsort()[-10:][::-1]]
                topics.append({'topic_id': topic_idx, 'words': top_words, 'weight': topic.max()})
            return topics
        except Exception as e:
            st.error(f"LDAã‚¨ãƒ©ãƒ¼: {e}")
            return []

    def sentiment_analysis(self, text):
        if not text:
            return {'polarity': 0, 'sentiment': 'neutral'}
        positive_words = ['è‰¯ã„', 'ã‚ˆã„', 'å¬‰ã—ã„', 'ã†ã‚Œã—ã„', 'æ¥½ã—ã„', 'æº€è¶³', 'åŠ©ã‹ã‚‹', 'å……å®Ÿ']
        negative_words = ['å›°ã‚‹', 'èª²é¡Œ', 'ä¸æº€', 'å¤§å¤‰', 'ç–²ã‚Œã‚‹', 'å•é¡Œ', 'ã‚¹ãƒˆãƒ¬ã‚¹']
        
        tokens = self.morphological_analysis(text)
        
        pos_count = sum(1 for word in tokens if word in positive_words)
        neg_count = sum(1 for word in tokens if word in negative_words)
        
        if pos_count > neg_count:
            return {'polarity': 1, 'sentiment': 'positive'}
        elif neg_count > pos_count:
            return {'polarity': -1, 'sentiment': 'negative'}
        elif pos_count > 0 and neg_count > 0:
            return {'polarity': 0, 'sentiment': 'ãƒŸãƒƒã‚¯ã‚¹'}
        else:
            return {'polarity': 0, 'sentiment': 'ä¸­ç«‹'}

    def classify_request_type(self, text):
        if not text:
            return 'unknown'
        
        tokens = self.morphological_analysis(text)
        
        maintain_keywords = ['ç¶­æŒ', 'ç¶™ç¶š', 'ç¾çŠ¶', 'ä¿æŒ', 'è‰¯ã„', 'ã‚ˆã„', 'æº€è¶³', 'å¬‰ã—ã„']
        enhance_keywords = ['å¼·åŒ–', 'å‘ä¸Š', 'ç™ºå±•', 'æ‹¡å¤§', 'æˆé•·', 'é€²æ­©', 'ç™ºé”', 'ã•ã‚‰ã«']
        improve_keywords = ['æ”¹å–„', 'èª²é¡Œ', 'å•é¡Œ', 'å›°ã‚‹', 'é›£ã—ã„', 'è¦‹ç›´ã—', 'å¤‰æ›´', 'ä¿®æ­£']
        
        scores = {
            'maintain': sum(1 for word in tokens if word in maintain_keywords),
            'enhance': sum(1 for word in tokens if word in enhance_keywords),
            'improve': sum(1 for word in tokens if word in improve_keywords)
        }
        
        if all(score == 0 for score in scores.values()):
            return 'maintain'
            
        max_cat = max(scores, key=scores.get)
        return max_cat

    def collocation_analysis(self, texts):
        all_words = []
        for text in texts:
            if text:
                all_words.extend(self.morphological_analysis(text))
        
        if len(all_words) < 2:
            return []
            
        bigrams = [(all_words[i], all_words[i+1]) for i in range(len(all_words) - 1)]
        
        return Counter(bigrams).most_common(10)

    def create_network_graph(self, collocations):
        G = nx.Graph()
        if not collocations:
            return None
        for (w1, w2), count in collocations:
            G.add_edge(w1, w2, weight=count)
        return G if G.number_of_nodes() > 0 else None

    def generate_wordcloud_image(self, text_data, file_name, width=800, height=400):
        if not text_data:
            st.warning(f"{file_name} ã®ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”¨ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚")
            return None

        wc = WordCloud(
            font_path=self.wc_font_path,
            width=width,
            height=height,
            background_color="black",
            max_words=100,
            min_font_size=10,
            collocations=False
        )
        
        word_counts = Counter(text_data)
        wc.generate_from_frequencies(word_counts)

        fig, ax = plt.subplots(figsize=(width/100, height/100), dpi=100)
        ax.imshow(wc, interpolation='bilinear')
        ax.axis("off")
        ax.set_title(file_name, fontproperties=font_prop, fontsize=18, color='white')
        
        return fig

    def draw_network_graph_matplotlib(self, G, title="å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚°ãƒ©ãƒ•"):
        if G is None or G.number_of_nodes() == 0:
            st.warning(f"{title} ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ãƒãƒ¼ãƒ‰ãŒä¸è¶³ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
            return None

        pos = nx.spring_layout(G, k=0.5, iterations=50, seed=42)
        
        fig, ax = plt.subplots(figsize=(12, 10))
        
        weights = [G[u][v]['weight'] for u,v in G.edges()]
        if weights:
            max_weight = max(weights)
            edge_widths = [w / max_weight * 4.5 + 0.5 for w in weights] 
        else:
            edge_widths = [1] * len(G.edges())

        nx.draw_networkx_edges(G, pos, ax=ax, width=edge_widths, alpha=0.5, edge_color='gray')

        node_sizes = [G.degree(node) * 200 + 500 for node in G.nodes()]
        
        nx.draw_networkx_nodes(G, pos, ax=ax, node_size=node_sizes, node_color='skyblue', alpha=0.9)

        nx.draw_networkx_labels(G, pos, font_size=10, font_color='black', font_family=font_prop.get_name())

        ax.set_title(title, fontproperties=font_prop, fontsize=18)
        ax.axis('off')
        
        return fig


    @st.cache_data(hash_funcs={pd.DataFrame: lambda _: None})
    def analyze_data(_self, df_input, top_n_keywords):
        _self.df = df_input.copy()
        _self.question_cols = [col for col in _self.df.columns if re.match(r'Q\d+å›ç­”', col)]

        st.write("### è‡ªç„¶è¨€èªå‡¦ç†åˆ†æä¸­...")

        # å…¨ä½“åˆ†æã®ãŸã‚ã®ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã¨å‰å‡¦ç†
        all_combined_texts_for_overall_analysis = []
        # å„å›ç­”è¡Œã®ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä¿æŒã™ã‚‹ãŸã‚ã®ãƒªã‚¹ãƒˆ
        overall_original_indices = [] 
        for idx, row in _self.df.iterrows():
            combined_row_text = []
            for col_name in _self.question_cols:
                preprocessed_text = _self.preprocess_text(row[col_name])
                if preprocessed_text:
                    combined_row_text.append(preprocessed_text)
            
            if combined_row_text: # ä½•ã‚‰ã‹ã®æœ‰åŠ¹ãªãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚‹å ´åˆã®ã¿è¿½åŠ 
                all_combined_texts_for_overall_analysis.append(" ".join(combined_row_text))
                overall_original_indices.append(idx) # å…ƒã®è¡Œã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä¿å­˜

        # æœ‰åŠ¹ãªå›ç­”ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        valid_overall_texts_with_indices = [
            (overall_original_indices[i], text) 
            for i, text in enumerate(all_combined_texts_for_overall_analysis) 
            if text
        ]
        
        st.write(f"å…¨ä½“ã§ã®æœ‰åŠ¹å›ç­”æ•°: {len(valid_overall_texts_with_indices)}")

        analysis_results = {}
        analysis_results['overall'] = {}
        
        overall_keyword_occurrence_counts = Counter()
        overall_word_list_for_wordcloud = [] # ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”¨ã«å…¨ã¦ã®å½¢æ…‹ç´ è§£æã•ã‚ŒãŸå˜èªã‚’ä¿æŒ

        # ãƒ›ãƒãƒ¼æƒ…å ±è¡¨ç¤ºã®ãŸã‚ã«ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ç´ã¥ã‘ãŸDataFrameã‚’ä½œæˆ
        overall_detailed_keyword_data = []

        for original_idx, text_content in valid_overall_texts_with_indices:
            morphed_words = _self.morphological_analysis(text_content)
            overall_word_list_for_wordcloud.extend(morphed_words) # ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”¨

            unique_words_in_text = set(morphed_words) # å›ç­”å†…ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            for word in unique_words_in_text:
                overall_keyword_occurrence_counts[word] += 1
                overall_detailed_keyword_data.append({
                    'keyword': word,
                    'original_text': _self.df.loc[original_idx, _self.question_cols].astype(str).str.cat(sep=" "), # å›ç­”å…¨ã¦ã‚’é€£çµ
                    'original_row_index': original_idx
                })
                
        analysis_results['overall']['word_list'] = overall_word_list_for_wordcloud # ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”¨
        analysis_results['overall']['top_words'] = overall_keyword_occurrence_counts.most_common(top_n_keywords)
        analysis_results['overall']['detailed_keywords_df'] = pd.DataFrame(overall_detailed_keyword_data)

        analysis_results['overall']['tfidf_keywords'] = _self.extract_keywords_tfidf([text for _, text in valid_overall_texts_with_indices])

        analysis_results['overall']['topics'] = _self.topic_modeling_lda([text for _, text in valid_overall_texts_with_indices])
        
        overall_sentiments = [_self.sentiment_analysis(t) for _, t in valid_overall_texts_with_indices]
        analysis_results['overall']['sentiments'] = Counter([s['sentiment'] for s in overall_sentiments])

        classifications = []
        for i, row in _self.df.iterrows():
            combined_text_parts = []
            for col in _self.question_cols:
                combined_text_parts.append(_self.preprocess_text(row[col]))
            classifications.append(_self.classify_request_type(" ".join(combined_text_parts).strip()))

        temp_df = _self.df.copy()
        temp_df['classification'] = classifications 
        analysis_results['overall']['classifications'] = Counter(classifications)
        
        _self.df = temp_df

        analysis_results['overall']['collocations'] = _self.collocation_analysis([text for _, text in valid_overall_texts_with_indices])

        has_org = 'çµ„ç¹”' in _self.df.columns and not _self.df['çµ„ç¹”'].empty
        has_gender = 'æ€§åˆ¥' in _self.df.columns and not _self.df['æ€§åˆ¥'].empty

        if has_org:
            org_analysis = _self.df.groupby('çµ„ç¹”')['classification'].value_counts().unstack(fill_value=0)
            analysis_results['overall']['org_analysis'] = org_analysis
        else:
            analysis_results['overall']['org_analysis'] = pd.DataFrame()

        if has_gender:
            gender_analysis = _self.df.groupby('æ€§åˆ¥')['classification'].value_counts().unstack(fill_value=0)
            analysis_results['overall']['gender_analysis'] = gender_analysis
        else:
            analysis_results['overall']['gender_analysis'] = pd.DataFrame()


        analysis_results['questions'] = {}
        for q_col in _self.question_cols:
            st.write(f"--- è¨­å• '{q_col}' ã®åˆ†æä¸­ ---")
            
            # è¨­å•ã”ã¨ã®æœ‰åŠ¹ãªãƒ†ã‚­ã‚¹ãƒˆã¨ãã®å…ƒã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä¿æŒ
            q_texts_with_indices = []
            for idx, row in _self.df.iterrows():
                preprocessed_text = _self.preprocess_text(row[q_col])
                if preprocessed_text:
                    q_texts_with_indices.append((idx, preprocessed_text))

            st.write(f"è¨­å• '{q_col}' ã®æœ‰åŠ¹å›ç­”æ•°: {len(q_texts_with_indices)}")

            analysis_results['questions'][q_col] = {}
            
            analysis_results['questions'][q_col]['keywords'] = _self.extract_keywords_tfidf([text for _, text in q_texts_with_indices])
            
            q_sentiments = [_self.sentiment_analysis(t) for _, t in q_texts_with_indices]
            analysis_results['questions'][q_col]['sentiments'] = Counter([s['sentiment'] for s in q_sentiments])
            
            analysis_results['questions'][q_col]['collocations'] = _self.collocation_analysis([text for _, text in q_texts_with_indices])
            
            q_keyword_occurrence_counts = Counter()
            q_word_list_for_wordcloud = [] # ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”¨ã«å…¨ã¦ã®å½¢æ…‹ç´ è§£æã•ã‚ŒãŸå˜èªã‚’ä¿æŒ

            # è¨­å•ã”ã¨ã®è©³ç´°ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰DataFrame (ãƒ›ãƒãƒ¼æƒ…å ±ç”¨)
            q_detailed_keyword_data = []

            for original_idx, text_content in q_texts_with_indices:
                morphed_words = _self.morphological_analysis(text_content)
                q_word_list_for_wordcloud.extend(morphed_words) # ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”¨

                unique_words_in_text = set(morphed_words) # å›ç­”å†…ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
                for word in unique_words_in_text: # ä¿®æ­£æ¸ˆã¿
                    q_keyword_occurrence_counts[word] += 1
                    q_detailed_keyword_data.append({
                        'keyword': word,
                        'original_text': _self.df.loc[original_idx, q_col], # ç‰¹å®šã®è³ªå•ã®å›ç­”ãƒ†ã‚­ã‚¹ãƒˆ
                        'original_row_index': original_idx
                    })

            analysis_results['questions'][q_col]['word_list'] = q_word_list_for_wordcloud # ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”¨
            analysis_results['questions'][q_col]['top_words'] = q_keyword_occurrence_counts.most_common(top_n_keywords)
            analysis_results['questions'][q_col]['detailed_keywords_df'] = pd.DataFrame(q_detailed_keyword_data)
        
        return analysis_results, _self.df


def main_app():
    st.set_page_config(layout="wide")
    st.title("ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆè‡ªç”±è¨˜è¿°åˆ†æã‚¢ãƒ—ãƒª")

    uploaded_file = st.sidebar.file_uploader("CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„", type=["csv"])

    if 'analysis_results' not in st.session_state:
        st.session_state['analysis_results'] = None
    if 'df' not in st.session_state:
        st.session_state['df'] = None
    if 'analyzer' not in st.session_state:
        st.session_state['analyzer'] = None
    if 'selected_overall_keyword' not in st.session_state:
        st.session_state['selected_overall_keyword'] = None
    if 'selected_overall_keyword_indices' not in st.session_state:
        st.session_state['selected_overall_keyword_indices'] = []

    df = None
    if uploaded_file is not None:
        try:
            df = pd.read_csv(uploaded_file, encoding='utf-8')
            st.sidebar.success("ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ­£å¸¸ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã—ãŸã€‚")
            st.sidebar.dataframe(df.head())

            top_n_keywords = st.sidebar.slider(
                "è¡¨ç¤ºã™ã‚‹é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°",
                min_value=1,
                max_value=50,
                value=st.session_state.get('top_n_keywords', 10),
                step=1,
                key='top_n_slider'
            )
            st.session_state['top_n_keywords'] = top_n_keywords

            if st.sidebar.button("åˆ†æã‚’å®Ÿè¡Œ"):
                analyzer = SurveyNLPAnalyzer(df)
                analysis_results, updated_df = analyzer.analyze_data(df, top_n_keywords) 

                st.session_state['analysis_results'] = analysis_results
                st.session_state['df'] = updated_df
                st.session_state['analyzer'] = analyzer
                
                st.session_state['selected_overall_keyword'] = None
                st.session_state['selected_overall_keyword_indices'] = []

                temp_analyzer = SurveyNLPAnalyzer(updated_df)
                question_cols_for_reset = [col for col in updated_df.columns if re.match(r'Q\d+å›ç­”', col)]

                for col_name in question_cols_for_reset:
                    if f'selected_q_keyword_{col_name}' in st.session_state:
                        del st.session_state[f'selected_q_keyword_{col_name}']
                    if f'selected_q_keyword_indices_{col_name}' in st.session_state:
                        del st.session_state[f'selected_q_keyword_indices_{col_name}']
                
                st.rerun()

        except UnicodeDecodeError:
            st.error("CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãŒUTF-8ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚'shift_jis'ã¾ãŸã¯'cp932'ã‚’è©¦ã—ã¦ã¿ã¦ãã ã•ã„ã€‚")
            try:
                uploaded_file.seek(0)
                df = pd.read_csv(uploaded_file, encoding='shift_jis')
                st.sidebar.success("ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ­£å¸¸ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã—ãŸ (Shift-JIS)ã€‚")
                st.session_state['df'] = df
                st.info("Shift-JISã§èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚åˆ†æã‚’é–‹å§‹ã™ã‚‹ã«ã¯å†åº¦ã€Œåˆ†æã‚’å®Ÿè¡Œã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
            except Exception as e:
                st.error(f"Shift-JISã§ã®èª­ã¿è¾¼ã¿ã‚‚å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        except Exception as e:
            st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    else:
        st.info("CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦åˆ†æã‚’é–‹å§‹ã—ã¦ãã ã•ã„ã€‚")

    if st.session_state['analysis_results'] is not None and st.session_state['analyzer'] is not None:
        analysis_results = st.session_state['analysis_results']
        df = st.session_state['df']
        analyzer = st.session_state['analyzer']
        top_n_keywords_display = st.session_state.get('top_n_keywords', 10)

        st.header("âœ¨ å…¨ä½“åˆ†æçµæœ")

        col1, col2 = st.columns(2)

        with col1:
            st.subheader("è¦æœ›åˆ†é¡åˆ†å¸ƒ")
            classification_counts = analysis_results['overall'].get('classifications', Counter())
            if classification_counts and any(v > 0 for v in classification_counts.values()):
                labels = list(classification_counts.keys())
                values = list(classification_counts.values())
                fig_pie = px.pie(names=labels, values=values, title='è¦æœ›åˆ†é¡åˆ†å¸ƒ (å…¨ä½“)')
                st.plotly_chart(fig_pie, use_container_width=True)
            else:
                st.info("è¦æœ›åˆ†é¡ãƒ‡ãƒ¼ã‚¿ãªã— (å…¨ä½“)")

            st.subheader("æ„Ÿæƒ…åˆ†æçµæœ")
            sentiment_counts = analysis_results['overall'].get('sentiments', Counter())
            if sentiment_counts and any(sentiment_counts.values()):
                sent_df = pd.DataFrame(sentiment_counts.items(), columns=['æ„Ÿæƒ…', 'ä»¶æ•°'])
                fig_sentiment = px.bar(sent_df, x='æ„Ÿæƒ…', y='ä»¶æ•°', title='æ„Ÿæƒ…åˆ†æçµæœ (å…¨ä½“)')
                st.plotly_chart(fig_sentiment, use_container_width=True)
            else:
                st.info("æ„Ÿæƒ…åˆ†æãƒ‡ãƒ¼ã‚¿ãªã— (å…¨ä½“)")

            st.subheader("çµ„ç¹”åˆ¥è¦æœ›åˆ†é¡")
            org_analysis = analysis_results['overall'].get('org_analysis', pd.DataFrame())
            if not org_analysis.empty and not org_analysis.sum().sum() == 0:
                org_analysis_melted = org_analysis.reset_index().melt(id_vars='çµ„ç¹”', var_name='åˆ†é¡', value_name='ä»¶æ•°')
                fig_org = px.bar(org_analysis_melted, x='çµ„ç¹”', y='ä»¶æ•°', color='åˆ†é¡', title='çµ„ç¹”åˆ¥è¦æœ›åˆ†é¡ (å…¨ä½“)')
                st.plotly_chart(fig_org, use_container_width=True)
            else:
                st.info("çµ„ç¹”åˆ¥åˆ†é¡ãƒ‡ãƒ¼ã‚¿ãªã— (å…¨ä½“)")

        with col2:
            st.subheader(f"é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¸Šä½{top_n_keywords_display}ä»¶")
            overall_top_words_df = pd.DataFrame(analysis_results['overall'].get('top_words', []), columns=['ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰', 'å‡ºç¾å›æ•°'])
            if not overall_top_words_df.empty:
                detailed_df = analysis_results['overall']['detailed_keywords_df']
                
                hover_text_map = detailed_df.groupby('keyword')['original_text'].apply(
                    lambda x: "<br>- " + "<br>- ".join(x.unique())
                ).to_dict()

                overall_top_words_df['custom_hover_text'] = overall_top_words_df['ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰'].map(hover_text_map).fillna("è©²å½“ã™ã‚‹å›ç­”ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

                overall_top_words_df['original_indices_json'] = overall_top_words_df['ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰'].apply(
                    lambda k: json.dumps([int(idx) for idx in detailed_df[detailed_df['keyword'] == k]['original_row_index'].unique()]) # int()ã«å¤‰æ›
                )

                fig_keywords = px.bar(overall_top_words_df, x='å‡ºç¾å›æ•°', y='ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰', orientation='h', 
                                    title=f'é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¸Šä½{top_n_keywords_display} (å…¨ä½“)',
                                    custom_data=['custom_hover_text', 'original_indices_json'] 
                                ) 
                
                fig_keywords.update_traces(
                    hovertemplate="<b>%{y}</b><br>å‡ºç¾å›æ•°: %{x}<br>" +
                                  "<b>é–¢é€£ã™ã‚‹å›ç­”:</b><br>%{customdata[0]}" + 
                                  "<extra></extra>" 
                )
                
                selected_points = plotly_events(
                    fig_keywords, 
                    select_event=True, 
                    key="overall_keywords_plot" 
                )

                if selected_points and 'customdata' in selected_points[0]:
                    clicked_keyword = selected_points[0]['y'] 
                    clicked_row_indices_json = selected_points[0]['customdata'][1]
                    clicked_row_indices = json.loads(clicked_row_indices_json)
                    
                    st.session_state['selected_overall_keyword'] = clicked_keyword
                    st.session_state['selected_overall_keyword_indices'] = clicked_row_indices
                    
                    st.rerun()
                
                if st.session_state['selected_overall_keyword']:
                    st.subheader(f"é¸æŠã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: ã€{st.session_state['selected_overall_keyword']}ã€ ã®é–¢é€£å›ç­”è©³ç´°")
                    
                    selected_indices = list(set(st.session_state['selected_overall_keyword_indices']))
                    if not df.empty and selected_indices:
                        display_cols = [col for col in df.columns if re.match(r'Q\d+å›ç­”', col)]
                        if 'çµ„ç¹”' in df.columns: display_cols.append('çµ„ç¹”')
                        if 'æ€§åˆ¥' in df.columns: display_cols.append('æ€§åˆ¥')
                        display_cols = list(set(display_cols))

                        st.dataframe(df.loc[selected_indices, display_cols])
                    else:
                        st.info("é¸æŠã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«é–¢é€£ã™ã‚‹è©³ç´°ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            else:
                st.info(f"é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãªã— (å…¨ä½“) - ç¾åœ¨ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°è¨­å®š: {top_n_keywords_display}")

            st.subheader("æ€§åˆ¥åˆ¥è¦æœ›åˆ†é¡")
            gender_analysis = analysis_results['overall'].get('gender_analysis', pd.DataFrame())
            if not gender_analysis.empty and not gender_analysis.sum().sum() == 0:
                gender_analysis_melted = gender_analysis.reset_index().melt(id_vars='æ€§åˆ¥', var_name='åˆ†é¡', value_name='ä»¶æ•°')
                fig_gender = px.bar(gender_analysis_melted, x='æ€§åˆ¥', y='ä»¶æ•°', color='åˆ†é¡', title='æ€§åˆ¥åˆ¥è¦æœ›åˆ†é¡ (å…¨ä½“)')
                st.plotly_chart(fig_gender, use_container_width=True)
            else:
                st.info("æ€§åˆ¥åˆ†é¡ãƒ‡ãƒ¼ã‚¿ãªã— (å…¨ä½“)")
            
            st.subheader("ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ (å…¨ä½“)")
            overall_wc_image_fig = analyzer.generate_wordcloud_image(
                analysis_results['overall'].get('word_list', []), 
                'ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ (å…¨ä½“)', width=1000, height=500
            )
            if overall_wc_image_fig:
                st.pyplot(overall_wc_image_fig)
            else:
                st.info("ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ãƒ‡ãƒ¼ã‚¿ãªã— (å…¨ä½“)")
            
            st.subheader("å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚°ãƒ©ãƒ• (å…¨ä½“)")
            overall_collocations = analysis_results['overall'].get('collocations', [])
            overall_network_graph = analyzer.create_network_graph(overall_collocations)
            
            if overall_network_graph and overall_network_graph.number_of_nodes() > 0:
                fig_overall_network = analyzer.draw_network_graph_matplotlib(overall_network_graph, "å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚°ãƒ©ãƒ• (å…¨ä½“)")
                if fig_overall_network:
                    st.pyplot(fig_overall_network)
            else:
                st.info("å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚°ãƒ©ãƒ•ã®ãƒ‡ãƒ¼ã‚¿ãªã— (å…¨ä½“)ã€‚å…±èµ·èªãŒè¦‹ã¤ã‹ã‚‰ãªã„ã‹ã€ãƒãƒ¼ãƒ‰ãŒä½œæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")


        st.markdown("---")

        st.header("ğŸ“ è¨­å•åˆ¥åˆ†æçµæœ")

        if analyzer.question_cols: 
            for q_col in analyzer.question_cols:
                if f'selected_q_keyword_{q_col}' not in st.session_state:
                    st.session_state[f'selected_q_keyword_{q_col}'] = None
                if f'selected_q_keyword_indices_{q_col}' not in st.session_state:
                    st.session_state[f'selected_q_keyword_indices_{q_col}'] = []

                st.subheader(f"### {q_col} ã®åˆ†æçµæœ")
                q_results = analysis_results['questions'][q_col]

                col_q1, col_q2, col_q3 = st.columns(3)

                with col_q1:
                    st.write(f"#### é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¸Šä½{top_n_keywords_display}ä»¶")
                    q_top_words_df = pd.DataFrame(q_results.get('top_words', []), columns=['ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰', 'å‡ºç¾å›æ•°'])
                    if not q_top_words_df.empty:
                        detailed_df = q_results['detailed_keywords_df']
                        
                        hover_text_map_q = detailed_df.groupby('keyword')['original_text'].apply(
                            lambda x: "<br>- " + "<br>- ".join(x.unique())
                        ).to_dict()

                        q_top_words_df['custom_hover_text'] = q_top_words_df['ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰'].map(hover_text_map_q).fillna("è©²å½“ã™ã‚‹å›ç­”ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                        
                        q_top_words_df['original_indices_json'] = q_top_words_df['ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰'].apply(
                            lambda k: json.dumps([int(idx) for idx in detailed_df[detailed_df['keyword'] == k]['original_row_index'].unique()]) # int()ã«å¤‰æ›
                        )

                        fig_q_keywords = px.bar(q_top_words_df, x='å‡ºç¾å›æ•°', y='ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰', orientation='h',
                                                title=f'é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¸Šä½{top_n_keywords_display} ({q_col})',
                                                custom_data=['custom_hover_text', 'original_indices_json'] 
                                            ) 

                        fig_q_keywords.update_traces(
                            hovertemplate="<b>%{y}</b><br>å‡ºç¾å›æ•°: %{x}<br>" +
                                          "<b>é–¢é€£ã™ã‚‹å›ç­”:</b><br>%{customdata[0]}" + 
                                          "<extra></extra>"
                        )
                        
                        selected_points_q = plotly_events(
                            fig_q_keywords, 
                            select_event=True, 
                            key=f"q_{q_col}_keywords_plot" 
                        )

                        if selected_points_q and 'customdata' in selected_points_q[0]:
                            clicked_keyword_q = selected_points_q[0]['y'] 
                            clicked_row_indices_json_q = selected_points_q[0]['customdata'][1]
                            clicked_row_indices_q = json.loads(clicked_row_indices_json_q)
                            
                            st.session_state[f'selected_q_keyword_{q_col}'] = clicked_keyword_q
                            st.session_state[f'selected_q_keyword_indices_{q_col}'] = clicked_row_indices_q
                            
                            st.rerun()
                        
                        if st.session_state.get(f'selected_q_keyword_{q_col}'):
                            with st.expander(f"é¸æŠã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: ã€{st.session_state[f'selected_q_keyword_{q_col}']}ã€ ã®é–¢é€£å›ç­”è©³ç´°"):
                                selected_indices_q = list(set(st.session_state[f'selected_q_keyword_indices_{q_col}']))
                                if not df.empty and selected_indices_q:
                                    display_cols_q = [q_col]
                                    if 'çµ„ç¹”' in df.columns: display_cols_q.append('çµ„ç¹”')
                                    if 'æ€§åˆ¥' in df.columns: display_cols_q.append('æ€§åˆ¥')
                                    display_cols_q = list(set(display_cols_q))
                                    
                                    st.dataframe(df.loc[selected_indices_q, display_cols_q])
                                else:
                                    st.info("é¸æŠã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«é–¢é€£ã™ã‚‹è©³ç´°ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                    else:
                        st.info(f"é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãªã— ({q_col}) - ç¾åœ¨ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°è¨­å®š: {top_n_keywords_display}")

                with col_q2:
                    st.write("#### æ„Ÿæƒ…åˆ†æçµæœ")
                    q_sentiment_counts = q_results.get('sentiments', Counter())
                    if q_sentiment_counts and any(q_sentiment_counts.values()):
                        q_sent_df = pd.DataFrame(q_sentiment_counts.items(), columns=['æ„Ÿæƒ…', 'ä»¶æ•°'])
                        fig_q_sentiment = px.bar(q_sent_df, x='æ„Ÿæƒ…', y='ä»¶æ•°', title=f'æ„Ÿæƒ…åˆ†æçµæœ ({q_col})')
                        st.plotly_chart(fig_q_sentiment, use_container_width=True)
                    else:
                        st.info(f"æ„Ÿæƒ…åˆ†æãƒ‡ãƒ¼ã‚¿ãªã— ({q_col})")

                with col_q3:
                    st.write("#### ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰")
                    q_wc_image_fig = analyzer.generate_wordcloud_image(
                        q_results.get('word_list', []), 
                        f'ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ ({q_col})', width=800, height=400
                    )
                    if q_wc_image_fig:
                        st.pyplot(q_wc_image_fig)
                    else:
                        st.info(f"ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ãƒ‡ãƒ¼ã‚¿ãªã— ({q_col})")
                    
                    st.write("#### å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚°ãƒ©ãƒ•")
                    q_collocations = q_results.get('collocations', [])
                    q_network_graph = analyzer.create_network_graph(q_collocations)
                    
                    if q_network_graph and q_network_graph.number_of_nodes() > 0:
                        fig_q_network = analyzer.draw_network_graph_matplotlib(q_network_graph, f"å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚°ãƒ©ãƒ• ({q_col})")
                        if fig_q_network:
                            st.pyplot(fig_q_network)
                    else:
                        st.info(f"å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚°ãƒ©ãƒ•ã®ãƒ‡ãƒ¼ã‚¿ãªã— ({q_col})ã€‚å…±èµ·èªãŒè¦‹ã¤ã‹ã‚‰ãªã„ã‹ã€ãƒãƒ¼ãƒ‰ãŒä½œæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")


                st.markdown("---") 
        else:
            st.info("åˆ†æå¯¾è±¡ã®è¨­å•å›ç­”åˆ—ï¼ˆQã€‡ã€‡å›ç­”ï¼‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚CSVãƒ•ã‚¡ã‚¤ãƒ«ã®åˆ—åã‚’ã”ç¢ºèªãã ã•ã„ã€‚")


if __name__ == '__main__':
    main_app()