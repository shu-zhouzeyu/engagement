import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import platform
import os
from matplotlib import font_manager, rcParams

# Plotly Expressã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import plotly.express as px

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š (WordCloudã¨Matplotlibç”¨)
if platform.system() == 'Windows':
    font_path = 'C:/Windows/Fonts/meiryo.ttc'
elif platform.system() == 'Darwin':
    font_path = '/System/Library/Fonts/ãƒ’ãƒ©ã‚®ãƒè§’ã‚´ã‚·ãƒƒã‚¯ W3.ttc'
else:
    # Linux (ä¾‹: Ubuntuã®å ´åˆ) ã®ãƒ•ã‚©ãƒ³ãƒˆãƒ‘ã‚¹
    font_path = '/usr/share/fonts/truetype/vlgothic/VL-Gothic-Regular.ttf'
    # ä»–ã®Linuxãƒ‡ã‚£ã‚¹ãƒˆãƒªãƒ“ãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã®å ´åˆã€é©å®œå¤‰æ›´ã—ã¦ãã ã•ã„

# font_prop ã¯ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¨ã—ã¦ä¿æŒã—ã€Streamlitã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚‚åˆ©ç”¨
font_prop = None
if os.path.exists(font_path):
    font_prop = font_manager.FontProperties(fname=font_path)
    rcParams['font.family'] = font_prop.get_name()
else:
    st.warning(f"æŒ‡å®šã•ã‚ŒãŸæ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆ '{font_path}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ä»£æ›¿ãƒ•ã‚©ãƒ³ãƒˆã‚’æ¤œç´¢ã—ã¾ã™ã€‚")
    font_files = font_manager.findSystemFonts(fontpaths=None)
    found_japanese_font = False
    for f in font_files:
        if "japanese" in f.lower() or "gothic" in f.lower() or "meiryo" in f.lower() or "hiragino" in f.lower() or "noto" in f.lower():
            font_path = f
            font_prop = font_manager.FontProperties(fname=font_path)
            rcParams['font.family'] = font_prop.get_name()
            st.info(f"ä»£æ›¿æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆ '{font_prop.get_name()}' ã‚’è¨­å®šã—ã¾ã—ãŸã€‚")
            found_japanese_font = True
            break
    if not found_japanese_font:
        st.error("ä»£æ›¿ã®æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã‚‚è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚æ–‡å­—åŒ–ã‘ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
        rcParams['font.family'] = ['sans-serif']

warnings.filterwarnings('ignore')

from collections import Counter
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation
import networkx as nx
from janome.tokenizer import Tokenizer
import re
from itertools import combinations
import json

class SurveyNLPAnalyzer:
    def __init__(self, df):
        self.df = df.copy() # ã‚ªãƒªã‚¸ãƒŠãƒ«DataFrameã®ã‚³ãƒ”ãƒ¼ã‚’æ“ä½œ
        self.tokenizer = Tokenizer()
        self.analysis_results = {}
        self.question_cols = [col for col in self.df.columns if re.match(r'Q\d+å›ç­”', col)]

        self.non_specific_patterns = [
            r'ã‚ã‹ã‚‰ãªã„', r'ç‰¹ã«ãªã—', r'ä¸æ˜', r'ãªã—',
            r'ç‰¹ã«.*ãªã„', r'ã‚ˆãã‚ã‹ã‚‰ãªã„', r'^\s*$', r'ãªã„'
        ]
        self.excluded_verbs = {'ã™ã‚‹', 'ãªã‚‹', 'ã„ã‚‹', 'ã‚‰ã‚Œã‚‹', 'æ„Ÿã˜ã‚‹', 'ã‚„ã™ã„', 'ã›ã‚‹', 'ã‚ã‚‹', 'ã„ã†','è‡ªåˆ†'}
        
        self.wc_font_path = font_path # Word Cloudç”¨ã®ãƒ•ã‚©ãƒ³ãƒˆãƒ‘ã‚¹

    def preprocess_text(self, text):
        if pd.isna(text):
            return ""
        text_str = str(text).lower()
        for pattern in self.non_specific_patterns:
            if re.search(pattern, text_str):
                return ""
        text_str = re.sub(r'[ï¼!ï¼Ÿ?ã€‚ï¼ã€ï¼Œ\s\tã€€]+', ' ', text_str)
        text_str = re.sub(r'[^\wã-ã‚“ã‚¡-ãƒ³ä¸€-é¾¥a-zA-Z0-9]', '', text_str)
        return text_str.strip()

    def morphological_analysis(self, text):
        if not text:
            return []
        words = []
        for token in self.tokenizer.tokenize(text):
            base_form = token.base_form
            pos = token.part_of_speech.split(',')[0]
            
            if pos in ['åè©', 'å‹•è©', 'å½¢å®¹è©']:
                if len(base_form) > 1 and base_form not in self.excluded_verbs:
                    words.append(base_form)
        return words

    def extract_keywords_tfidf(self, texts, top_n=10):
        processed_texts_for_vectorizer = [" ".join(self.morphological_analysis(text)) for text in texts if text]
        
        if not processed_texts_for_vectorizer:
            return []
        
        vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2), min_df=1, max_df=0.8)
        tfidf_matrix = vectorizer.fit_transform(processed_texts_for_vectorizer)
        
        feature_names = vectorizer.get_feature_names_out()
        scores = tfidf_matrix.sum(axis=0).A1
        keywords = [(feature_names[i], scores[i]) for i in range(len(feature_names))]
        return sorted(keywords, key=lambda x: x[1], reverse=True)[:top_n]

    def topic_modeling_lda(self, texts, n_topics=5):
        processed_texts_for_vectorizer = [" ".join(self.morphological_analysis(text)) for text in texts if text]

        if len(processed_texts_for_vectorizer) < 2:
            return []
        try:
            vectorizer = TfidfVectorizer(max_features=100, min_df=1, max_df=0.8)
            tfidf_matrix = vectorizer.fit_transform(processed_texts_for_vectorizer)
            
            feature_names = vectorizer.get_feature_names_out()
            lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)
            lda.fit(tfidf_matrix)
            topics = []
            for topic_idx, topic in enumerate(lda.components_):
                top_words = [feature_names[i] for i in topic.argsort()[-10:][::-1]]
                topics.append({'topic_id': topic_idx, 'words': top_words, 'weight': topic.max()})
            return topics
        except Exception as e:
            st.error(f"LDAã‚¨ãƒ©ãƒ¼: {e}")
            return []

    def sentiment_analysis(self, text):
        if not text:
            return {'polarity': 0, 'sentiment': 'neutral'}
        positive_words = ['è‰¯ã„', 'ã‚ˆã„', 'å¬‰ã—ã„', 'ã†ã‚Œã—ã„', 'æ¥½ã—ã„', 'æº€è¶³', 'åŠ©ã‹ã‚‹', 'å……å®Ÿ']
        negative_words = ['å›°ã‚‹', 'èª²é¡Œ', 'ä¸æº€', 'å¤§å¤‰', 'ç–²ã‚Œã‚‹', 'å•é¡Œ', 'ã‚¹ãƒˆãƒ¬ã‚¹']
        
        tokens = self.morphological_analysis(text)
        
        pos_count = sum(1 for word in tokens if word in positive_words)
        neg_count = sum(1 for word in tokens if word in negative_words)
        
        if pos_count > neg_count:
            return {'polarity': 1, 'sentiment': 'positive'}
        elif neg_count > pos_count:
            return {'polarity': -1, 'sentiment': 'negative'}
        elif pos_count > 0 and neg_count > 0:
            return {'polarity': 0, 'sentiment': 'ãƒŸãƒƒã‚¯ã‚¹'}
        else:
            return {'polarity': 0, 'sentiment': 'ä¸­ç«‹'}

    def classify_request_type(self, text):
        if not text:
            return 'unknown'
        
        tokens = self.morphological_analysis(text)
        
        maintain_keywords = ['ç¶­æŒ', 'ç¶™ç¶š', 'ç¾çŠ¶', 'ä¿æŒ', 'è‰¯ã„', 'ã‚ˆã„', 'æº€è¶³', 'å¬‰ã—ã„']
        enhance_keywords = ['å¼·åŒ–', 'å‘ä¸Š', 'ç™ºå±•', 'æ‹¡å¤§', 'æˆé•·', 'é€²æ­©', 'ç™ºé”', 'ã•ã‚‰ã«']
        improve_keywords = ['æ”¹å–„', 'èª²é¡Œ', 'å•é¡Œ', 'å›°ã‚‹', 'é›£ã—ã„', 'è¦‹ç›´ã—', 'å¤‰æ›´', 'ä¿®æ­£']
        
        scores = {
            'maintain': sum(1 for word in tokens if word in maintain_keywords),
            'enhance': sum(1 for word in tokens if word in enhance_keywords),
            'improve': sum(1 for word in tokens if word in improve_keywords)
        }
        
        if all(score == 0 for score in scores.values()):
            return 'maintain'
            
        max_cat = max(scores, key=scores.get)
        return max_cat

    def collocation_analysis(self, texts):
        all_words = []
        for text in texts:
            if text:
                all_words.extend(self.morphological_analysis(text))
        
        if len(all_words) < 2:
            return []
            
        bigrams = [(all_words[i], all_words[i+1]) for i in range(len(all_words) - 1)]
        
        return Counter(bigrams).most_common(10)

    def create_network_graph(self, collocations):
        G = nx.Graph()
        if not collocations:
            return None
        for (w1, w2), count in collocations:
            G.add_edge(w1, w2, weight=count)
        return G if G.number_of_nodes() > 0 else None


    @st.cache_data # åˆ†æçµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã¦é«˜é€ŸåŒ–
    def analyze_data(_self, df_input, top_n_keywords): # top_n_keywords ã‚’å¼•æ•°ã«è¿½åŠ 
        _self.df = df_input.copy() # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸdf_inputã‚’ä½¿ç”¨
        _self.question_cols = [col for col in _self.df.columns if re.match(r'Q\d+å›ç­”', col)]

        st.write("### è‡ªç„¶è¨€èªå‡¦ç†åˆ†æä¸­...")

        all_combined_texts_for_overall_analysis = []
        for col_name in _self.question_cols:
            all_combined_texts_for_overall_analysis.extend(_self.df[col_name].apply(_self.preprocess_text).tolist())
        valid_overall_texts = [text for text in all_combined_texts_for_overall_analysis if text]
        
        st.write(f"å…¨ä½“ã§ã®æœ‰åŠ¹å›ç­”æ•°: {len(valid_overall_texts)}")

        analysis_results = {}
        analysis_results['overall'] = {}
        
        # WordCloudã¨Plotly Bar Chartç”¨ã®ç”Ÿã®å˜èªãƒªã‚¹ãƒˆã¨DataFrameã‚’ä¿å­˜
        overall_word_list = []
        for text in valid_overall_texts:
            overall_word_list.extend(_self.morphological_analysis(text))
        analysis_results['overall']['word_list'] = overall_word_list
        # â˜…â˜…â˜… ä¿®æ­£ç‚¹: top_n_keywords ã‚’ Counter.most_common ã«æ¸¡ã™ â˜…â˜…â˜…
        analysis_results['overall']['top_words'] = Counter(overall_word_list).most_common(top_n_keywords)
        
        # ãƒ›ãƒãƒ¼æƒ…å ±è¡¨ç¤ºã®ãŸã‚ã«ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ç´ã¥ã‘ãŸDataFrameã‚’ä½œæˆ
        keyword_data = []
        for col in _self.question_cols:
            for idx, row in _self.df.iterrows():
                preprocessed_text = _self.preprocess_text(row[col])
                if preprocessed_text:
                    words = _self.morphological_analysis(preprocessed_text)
                    for word in words:
                        keyword_data.append({
                            'keyword': word,
                            'original_text': row[col] # â˜…â˜…â˜… ä¿®æ­£ç‚¹: ãƒ›ãƒãƒ¼ã«è¡¨ç¤ºã™ã‚‹å†…å®¹ã‚’original_textã®ã¿ã«ã™ã‚‹ â˜…â˜…â˜…
                        })
        analysis_results['overall']['detailed_keywords_df'] = pd.DataFrame(keyword_data)

        # TF-IDFã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ (ã“ã‚Œã¯ä»¥å‰ã®æ£’ã‚°ãƒ©ãƒ•ç”¨ã§ã€Plotlyæ£’ã‚°ãƒ©ãƒ•ã§ã¯'top_words'ã‚’ä½¿ã†)
        analysis_results['overall']['tfidf_keywords'] = _self.extract_keywords_tfidf(valid_overall_texts)

        analysis_results['overall']['topics'] = _self.topic_modeling_lda(valid_overall_texts)
        
        overall_sentiments = [_self.sentiment_analysis(t) for t in valid_overall_texts]
        analysis_results['overall']['sentiments'] = Counter([s['sentiment'] for s in overall_sentiments])

        classifications = []
        for i, row in _self.df.iterrows():
            combined_text_parts = []
            for col in _self.question_cols:
                combined_text_parts.append(_self.preprocess_text(row[col]))
            classifications.append(_self.classify_request_type(" ".join(combined_text_parts).strip()))

        temp_df = _self.df.copy() # ä¸€æ™‚çš„ã«åˆ†é¡çµæœã‚’è¿½åŠ ã™ã‚‹ãŸã‚ã®ã‚³ãƒ”ãƒ¼
        temp_df['classification'] = classifications 
        analysis_results['overall']['classifications'] = Counter(classifications)
        
        _self.df = temp_df # åˆ†é¡çµæœã‚’å…ƒã®DataFrameã«åæ˜ ï¼ˆStreamlitã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†ã‚’è€ƒæ…®ï¼‰

        analysis_results['overall']['collocations'] = _self.collocation_analysis(valid_overall_texts)

        # å±æ€§åˆ¥åˆ†æ
        has_org = 'çµ„ç¹”' in _self.df.columns and not _self.df['çµ„ç¹”'].empty
        has_gender = 'æ€§åˆ¥' in _self.df.columns and not _self.df['æ€§åˆ¥'].empty

        if has_org:
            org_analysis = _self.df.groupby('çµ„ç¹”')['classification'].value_counts().unstack(fill_value=0)
            analysis_results['overall']['org_analysis'] = org_analysis
        else:
            analysis_results['overall']['org_analysis'] = pd.DataFrame()

        if has_gender:
            gender_analysis = _self.df.groupby('æ€§åˆ¥')['classification'].value_counts().unstack(fill_value=0)
            analysis_results['overall']['gender_analysis'] = gender_analysis
        else:
            analysis_results['overall']['gender_analysis'] = pd.DataFrame()


        analysis_results['questions'] = {}
        for q_col in _self.question_cols:
            st.write(f"--- è¨­å• '{q_col}' ã®åˆ†æä¸­ ---")
            q_texts = _self.df[q_col].apply(_self.preprocess_text).tolist()
            valid_q_texts = [text for text in q_texts if text]
            st.write(f"è¨­å• '{q_col}' ã®æœ‰åŠ¹å›ç­”æ•°: {len(valid_q_texts)}")

            analysis_results['questions'][q_col] = {}
            
            analysis_results['questions'][q_col]['keywords'] = _self.extract_keywords_tfidf(valid_q_texts)
            
            q_sentiments = [_self.sentiment_analysis(t) for t in valid_q_texts]
            analysis_results['questions'][q_col]['sentiments'] = Counter([s['sentiment'] for s in q_sentiments])
            
            analysis_results['questions'][q_col]['collocations'] = _self.collocation_analysis(valid_q_texts)
            
            q_word_list = [] # ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã¨Plotly Bar Chartç”¨ã«å˜èªãƒªã‚¹ãƒˆã‚’ä¿æŒ
            for text in valid_q_texts:
                q_word_list.extend(_self.morphological_analysis(text))
            # â˜…â˜…â˜… ä¿®æ­£ç‚¹: top_n_keywords ã‚’ Counter.most_common ã«æ¸¡ã™ â˜…â˜…â˜…
            analysis_results['questions'][q_col]['top_words'] = Counter(q_word_list).most_common(top_n_keywords)
            analysis_results['questions'][q_col]['word_list'] = q_word_list # ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”¨ã¨Plotlyãƒ›ãƒãƒ¼æƒ…å ±ç”¨

            # è¨­å•ã”ã¨ã®è©³ç´°ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰DataFrame (ãƒ›ãƒãƒ¼æƒ…å ±ç”¨)
            q_keyword_data = []
            for idx, row in _self.df.iterrows():
                preprocessed_text = _self.preprocess_text(row[q_col])
                if preprocessed_text:
                    words = _self.morphological_analysis(preprocessed_text)
                    for word in words:
                        q_keyword_data.append({
                            'keyword': word,
                            'original_text': row[q_col] # â˜…â˜…â˜… ä¿®æ­£ç‚¹: ãƒ›ãƒãƒ¼ã«è¡¨ç¤ºã™ã‚‹å†…å®¹ã‚’original_textã®ã¿ã«ã™ã‚‹ â˜…â˜…â˜…
                        })
            analysis_results['questions'][q_col]['detailed_keywords_df'] = pd.DataFrame(q_keyword_data)
        
        return analysis_results, _self.df # æ›´æ–°ã•ã‚ŒãŸdfã‚‚è¿”ã™


# Streamlitã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ¡ã‚¤ãƒ³é–¢æ•°
def main_app():
    st.set_page_config(layout="wide") # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚’ãƒ¯ã‚¤ãƒ‰ã«è¨­å®š
    st.title("ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆè‡ªç”±è¨˜è¿°åˆ†æã‚¢ãƒ—ãƒª")

    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ€ãƒ¼
    uploaded_file = st.sidebar.file_uploader("CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„", type=["csv"])

    df = None
    if uploaded_file is not None:
        try:
            # CSVã‚’èª­ã¿è¾¼ã‚€éš›ã«encodingã‚’æŒ‡å®š
            df = pd.read_csv(uploaded_file, encoding='utf-8')
            st.sidebar.success("ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ­£å¸¸ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã—ãŸã€‚")
            st.sidebar.dataframe(df.head()) # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®å†’é ­ã‚’è¡¨ç¤º

            # â˜…â˜…â˜… æ–°æ©Ÿèƒ½: é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼ â˜…â˜…â˜…
            # defaultã¯10ã€æœ€å°1ã€æœ€å¤§50ç¨‹åº¦ã€ã‚¹ãƒ†ãƒƒãƒ—1
            top_n_keywords = st.sidebar.slider(
                "è¡¨ç¤ºã™ã‚‹é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°",
                min_value=1,
                max_value=50,
                value=10,
                step=1
            )
            st.session_state['top_n_keywords'] = top_n_keywords # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿å­˜

            # Analyzeãƒœã‚¿ãƒ³
            if st.sidebar.button("åˆ†æã‚’é–‹å§‹"):
                analyzer = SurveyNLPAnalyzer(df)
                # â˜…â˜…â˜… ä¿®æ­£ç‚¹: analyze_data ã« top_n_keywords ã‚’æ¸¡ã™ â˜…â˜…â˜…
                analysis_results, updated_df = analyzer.analyze_data(df, top_n_keywords) 

                st.session_state['analysis_results'] = analysis_results
                st.session_state['df'] = updated_df # æ›´æ–°ã•ã‚ŒãŸDataFrameã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿å­˜
                st.session_state['analyzer'] = analyzer # analyzerã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚‚ä¿å­˜

        except UnicodeDecodeError:
            st.error("CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãŒUTF-8ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚'shift_jis'ã¾ãŸã¯'cp932'ã‚’è©¦ã—ã¦ã¿ã¦ãã ã•ã„ã€‚")
            try:
                uploaded_file.seek(0) # ãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚¤ãƒ³ã‚¿ã‚’å…ˆé ­ã«æˆ»ã™
                df = pd.read_csv(uploaded_file, encoding='shift_jis')
                st.sidebar.success("ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ­£å¸¸ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã—ãŸ (Shift-JIS)ã€‚")
                st.session_state['df'] = df # Session State ã« df ã‚’ä¿å­˜
                st.info("Shift-JISã§èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚åˆ†æã‚’é–‹å§‹ã™ã‚‹ã«ã¯å†åº¦ã€Œåˆ†æã‚’é–‹å§‹ã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
            except Exception as e:
                st.error(f"Shift-JISã§ã®èª­ã¿è¾¼ã¿ã‚‚å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        except Exception as e:
            st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    else:
        st.info("CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦åˆ†æã‚’é–‹å§‹ã—ã¦ãã ã•ã„ã€‚")

    if 'analysis_results' in st.session_state and 'analyzer' in st.session_state:
        analysis_results = st.session_state['analysis_results']
        df = st.session_state['df']
        analyzer = st.session_state['analyzer']
        # â˜…â˜…â˜… ä¿®æ­£ç‚¹: ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼ã®å€¤ã‚‚ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‹ã‚‰å–å¾— â˜…â˜…â˜…
        top_n_keywords_display = st.session_state.get('top_n_keywords', 10) # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã¯10

        # # ãƒ‡ãƒãƒƒã‚°ç”¨: ã‚·ãƒ³ãƒ—ãƒ«ãªã‚°ãƒ©ãƒ•ã§ãƒ›ãƒãƒ¼æ©Ÿèƒ½ã®å‹•ä½œç¢ºèª
        # st.header("--- ãƒ‡ãƒãƒƒã‚°ç”¨ãƒ†ã‚¹ãƒˆã‚°ãƒ©ãƒ• ---")
        # test_data = pd.DataFrame({
        #     'Category': ['A', 'B', 'C'],
        #     'Value': [10, 20, 50],
        #     'HoverInfo': ['ãƒ†ã‚¹ãƒˆè©³ç´°æƒ…å ±1', 'ãƒ†ã‚¹ãƒˆè©³ç´°æƒ…å ±2', 'ãƒ†ã‚¹ãƒˆè©³ç´°æƒ…å ±3'] # ç´”ç²‹ãªãƒ†ã‚­ã‚¹ãƒˆä¾‹
        # })

        # test_fig = px.bar(test_data, x='Category', y='Value', 
        #                   title="ãƒ†ã‚¹ãƒˆãƒãƒ¼ãƒãƒ£ãƒ¼ãƒˆ",
        #                   custom_data=[test_data['HoverInfo'].tolist()]) 

        # test_fig.update_traces(
        #     hovertemplate="<b>ã‚«ãƒ†ã‚´ãƒª:</b> %{x}<br>" +
        #                   "<b>å€¤:</b> %{y}<br>" +
        #                   "<b>è©³ç´°:</b> %{customdata}" + 
        #                   "<extra></extra>" # ã“ã‚Œã§ãƒ„ãƒ¼ãƒ«ãƒãƒƒãƒ—ã«ä½™è¨ˆãªTraceæƒ…å ±ãŒè¡¨ç¤ºã•ã‚Œãªããªã‚‹
        # )
        # st.plotly_chart(test_fig, use_container_width=True)
        # st.header("----------------------------")

        st.header("âœ¨ å…¨ä½“åˆ†æçµæœ")

        # å…¨ä½“çµæœã®è¡¨ç¤º (2åˆ—ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ)
        col1, col2 = st.columns(2)

        with col1:
            st.subheader("è¦æœ›åˆ†é¡åˆ†å¸ƒ")
            classification_counts = analysis_results['overall'].get('classifications', Counter())
            if classification_counts and any(v > 0 for v in classification_counts.values()):
                labels = list(classification_counts.keys())
                values = list(classification_counts.values())
                fig_pie = px.pie(names=labels, values=values, title='è¦æœ›åˆ†é¡åˆ†å¸ƒ (å…¨ä½“)')
                st.plotly_chart(fig_pie, use_container_width=True)
            else:
                st.info("è¦æœ›åˆ†é¡ãƒ‡ãƒ¼ã‚¿ãªã— (å…¨ä½“)")

            st.subheader("æ„Ÿæƒ…åˆ†æçµæœ")
            sentiment_counts = analysis_results['overall'].get('sentiments', Counter())
            if sentiment_counts and any(sentiment_counts.values()):
                sent_df = pd.DataFrame(sentiment_counts.items(), columns=['æ„Ÿæƒ…', 'ä»¶æ•°'])
                fig_sentiment = px.bar(sent_df, x='æ„Ÿæƒ…', y='ä»¶æ•°', title='æ„Ÿæƒ…åˆ†æçµæœ (å…¨ä½“)')
                st.plotly_chart(fig_sentiment, use_container_width=True)
            else:
                st.info("æ„Ÿæƒ…åˆ†æãƒ‡ãƒ¼ã‚¿ãªã— (å…¨ä½“)")

            st.subheader("çµ„ç¹”åˆ¥è¦æœ›åˆ†é¡")
            org_analysis = analysis_results['overall'].get('org_analysis', pd.DataFrame())
            if not org_analysis.empty and not org_analysis.sum().sum() == 0:
                # Plotlyã§ç©ã¿ä¸Šã’æ£’ã‚°ãƒ©ãƒ•
                org_analysis_melted = org_analysis.reset_index().melt(id_vars='çµ„ç¹”', var_name='åˆ†é¡', value_name='ä»¶æ•°')
                fig_org = px.bar(org_analysis_melted, x='çµ„ç¹”', y='ä»¶æ•°', color='åˆ†é¡', title='çµ„ç¹”åˆ¥è¦æœ›åˆ†é¡ (å…¨ä½“)')
                st.plotly_chart(fig_org, use_container_width=True)
            else:
                st.info("çµ„ç¹”åˆ¥åˆ†é¡ãƒ‡ãƒ¼ã‚¿ãªã— (å…¨ä½“)")

        with col2:
            st.subheader(f"é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¸Šä½{top_n_keywords_display}ä»¶") # ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼ã®å€¤ã‚’ã‚¿ã‚¤ãƒˆãƒ«ã«åæ˜ 
            overall_top_words_df = pd.DataFrame(analysis_results['overall'].get('top_words', []), columns=['ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰', 'å‡ºç¾å›æ•°'])
            if not overall_top_words_df.empty:
                detailed_df = analysis_results['overall']['detailed_keywords_df']
                
                # â˜…â˜…â˜… ä¿®æ­£ç‚¹: ãƒ›ãƒãƒ¼ãƒ†ã‚­ã‚¹ãƒˆã‚’ç´”ç²‹ãªoriginal_textã®ã¿ã«ã™ã‚‹ â˜…â˜…â˜…
                # å„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«é–¢é€£ã™ã‚‹original_textã‚’ã™ã¹ã¦çµåˆã™ã‚‹
                overall_top_words_df['custom_hover_text'] = overall_top_words_df['ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰'].apply(
                    lambda k: "-".join([
                        row['original_text'] # <-- ã“ã‚ŒãŒç´”ç²‹ãªCSVæŠ½å‡ºæ–‡è¨€
                        for _, row in detailed_df[detailed_df['keyword'] == k].iterrows()
                    ]) or "è©²å½“ã™ã‚‹å›ç­”ãŒã‚ã‚Šã¾ã›ã‚“ã€‚"
                )
                
                # --- ãƒ‡ãƒãƒƒã‚°ç”¨å‡ºåŠ› ---
                # st.write("--- Overall Custom Hover Text (Sample) ---")
                # st.dataframe(overall_top_words_df[['ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰', 'custom_hover_text']].head())
                # --- ãƒ‡ãƒãƒƒã‚°ç”¨å‡ºåŠ›çµ‚ã‚ã‚Š ---

                fig_keywords = px.bar(overall_top_words_df, x='å‡ºç¾å›æ•°', y='ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰', orientation='h', 
                                    title=f'é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¸Šä½{top_n_keywords_display} (å…¨ä½“)', # ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼ã®å€¤ã‚’ã‚¿ã‚¤ãƒˆãƒ«ã«åæ˜ 
                                    custom_data=[overall_top_words_df['custom_hover_text'].tolist()]) 
                
                fig_keywords.update_traces(
                    hovertemplate="<b>%{y}</b><br>å‡ºç¾å›æ•°: %{x}<br>" +
                                  "<b>é–¢é€£ã™ã‚‹å›ç­”:</b><br>%{customdata}" + 
                                  "<extra></extra>" 
                )
                
                st.plotly_chart(fig_keywords, use_container_width=True)
            else:
                st.info(f"é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãªã— (å…¨ä½“) - ç¾åœ¨ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°è¨­å®š: {top_n_keywords_display}")

            st.subheader("æ€§åˆ¥åˆ¥è¦æœ›åˆ†é¡")
            gender_analysis = analysis_results['overall'].get('gender_analysis', pd.DataFrame())
            if not gender_analysis.empty and not gender_analysis.sum().sum() == 0:
                gender_analysis_melted = gender_analysis.reset_index().melt(id_vars='æ€§åˆ¥', var_name='åˆ†é¡', value_name='ä»¶æ•°')
                fig_gender = px.bar(gender_analysis_melted, x='æ€§åˆ¥', y='ä»¶æ•°', color='åˆ†é¡', title='æ€§åˆ¥åˆ¥è¦æœ›åˆ†é¡ (å…¨ä½“)')
                st.plotly_chart(fig_gender, use_container_width=True)
            else:
                st.info("æ€§åˆ¥åˆ†é¡ãƒ‡ãƒ¼ã‚¿ãªã— (å…¨ä½“)")
            

        st.markdown("---")

        st.header("ğŸ“ è¨­å•åˆ¥åˆ†æçµæœ")

        # è¨­å•ã”ã¨ã®çµæœã‚’è¡¨ç¤º
        if analyzer.question_cols: 
            for q_col in analyzer.question_cols:
                st.subheader(f"### {q_col} ã®åˆ†æçµæœ")
                q_results = analysis_results['questions'][q_col]

                col_q1, col_q2, col_q3 = st.columns(3)

                with col_q1:
                    st.write(f"#### é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¸Šä½{top_n_keywords_display}ä»¶") # ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼ã®å€¤ã‚’ã‚¿ã‚¤ãƒˆãƒ«ã«åæ˜ 
                    q_top_words_df = pd.DataFrame(q_results.get('top_words', []), columns=['ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰', 'å‡ºç¾å›æ•°'])
                    if not q_top_words_df.empty:
                        detailed_df = q_results['detailed_keywords_df']
                        
                        # â˜…â˜…â˜… ä¿®æ­£ç‚¹: ãƒ›ãƒãƒ¼ãƒ†ã‚­ã‚¹ãƒˆã‚’ç´”ç²‹ãªoriginal_textã®ã¿ã«ã™ã‚‹ â˜…â˜…â˜…
                        q_top_words_df['custom_hover_text'] = q_top_words_df['ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰'].apply(
                            lambda k: "-".join([
                                row['original_text'] # <-- ã“ã‚ŒãŒç´”ç²‹ãªCSVæŠ½å‡ºæ–‡è¨€
                                for _, row in detailed_df[detailed_df['keyword'] == k].iterrows()
                            ]) or "è©²å½“ã™ã‚‹å›ç­”ãŒã‚ã‚Šã¾ã›ã‚“ã€‚"
                        )

                        # --- ãƒ‡ãƒãƒƒã‚°ç”¨å‡ºåŠ› ---
                        st.write(f"--- {q_col} Custom Hover Text (Sample) ---") 
                        st.dataframe(q_top_words_df[['ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰', 'custom_hover_text']].head())
                        # --- ãƒ‡ãƒãƒƒã‚°ç”¨å‡ºåŠ›çµ‚ã‚ã‚Š ---

                        fig_q_keywords = px.bar(q_top_words_df, x='å‡ºç¾å›æ•°', y='ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰', orientation='h',
                                                title=f'é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¸Šä½{top_n_keywords_display} ({q_col})', # ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼ã®å€¤ã‚’ã‚¿ã‚¤ãƒˆãƒ«ã«åæ˜ 
                                                custom_data=[q_top_words_df['custom_hover_text'].tolist()]) 

                        fig_q_keywords.update_traces(
                            hovertemplate="<b>%{y}</b><br>å‡ºç¾å›æ•°: %{x}<br>" +
                                          "<b>é–¢é€£ã™ã‚‹å›ç­”:</b><br>%{customdata}" + 
                                          "<extra></extra>"
                        )
                        st.plotly_chart(fig_q_keywords, use_container_width=True)
                    else:
                        st.info(f"é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãªã— ({q_col}) - ç¾åœ¨ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°è¨­å®š: {top_n_keywords_display}")

                with col_q2:
                    st.write("#### æ„Ÿæƒ…åˆ†æçµæœ")
                    q_sentiment_counts = q_results.get('sentiments', Counter())
                    if q_sentiment_counts and any(q_sentiment_counts.values()):
                        q_sent_df = pd.DataFrame(q_sentiment_counts.items(), columns=['æ„Ÿæƒ…', 'ä»¶æ•°'])
                        fig_q_sentiment = px.bar(q_sent_df, x='æ„Ÿæƒ…', y='ä»¶æ•°', title=f'æ„Ÿæƒ…åˆ†æçµæœ ({q_col})')
                        st.plotly_chart(fig_q_sentiment, use_container_width=True)
                    else:
                        st.info(f"æ„Ÿæƒ…åˆ†æãƒ‡ãƒ¼ã‚¿ãªã— ({q_col})")

                st.markdown("---") # å„è¨­å•ã®é–“ã«åŒºåˆ‡ã‚Šç·š
        else: # question_cols ãŒç©ºã®å ´åˆã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            st.info("åˆ†æå¯¾è±¡ã®è¨­å•å›ç­”åˆ—ï¼ˆQã€‡ã€‡å›ç­”ï¼‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚CSVãƒ•ã‚¡ã‚¤ãƒ«ã®åˆ—åã‚’ã”ç¢ºèªãã ã•ã„ã€‚")


if __name__ == '__main__':
    main_app()